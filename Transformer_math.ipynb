{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9cHa4n0aqNlzO5AVdqmyr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmepriyabrata/priyabrata_ai_python/blob/main/Transformer_math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJMxzv0iqCR4",
        "outputId": "127c5125-ac09-4ecd-fc9d-cf5cbe2333bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Embeddings:\n",
            " tensor([[1., 0., 1., 0.],\n",
            "        [0., 2., 0., 1.]])\n",
            "Queries (Q):\n",
            " tensor([[0.3883, 1.2941, 1.3072, 1.5628],\n",
            "        [1.4930, 1.6263, 1.5802, 1.5664]])\n",
            "Keys (K):\n",
            " tensor([[1.2488, 1.0988, 0.7671, 1.3177],\n",
            "        [1.5061, 0.6642, 1.5973, 2.1780]])\n",
            "Values (V):\n",
            " tensor([[1.1547, 0.7331, 0.9069, 1.1389],\n",
            "        [1.1839, 1.6739, 2.3724, 1.2075]])\n",
            "Attention Scores:\n",
            " tensor([[2.4845, 3.4681],\n",
            "        [3.4638, 4.6322]])\n",
            "Attention Weights:\n",
            " tensor([[0.2722, 0.7278],\n",
            "        [0.2372, 0.7628]])\n",
            "Attention Output:\n",
            " tensor([[1.1760, 1.4178, 1.9735, 1.1888],\n",
            "        [1.1770, 1.4508, 2.0248, 1.1912]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input embeddings (sequence length = 2, embedding size = 4)\n",
        "X = torch.tensor([[1.0, 0.0, 1.0, 0.0],\n",
        "                  [0.0, 2.0, 0.0, 1.0]])\n",
        "\n",
        "# Define the weight matrices (for simplicity, we use small random values)\n",
        "d_model = 4\n",
        "d_k = 4\n",
        "\n",
        "W_Q = torch.rand(d_model, d_k)\n",
        "W_K = torch.rand(d_model, d_k)\n",
        "W_V = torch.rand(d_model, d_k)\n",
        "\n",
        "# Compute Q, K, V\n",
        "Q = torch.matmul(X, W_Q)\n",
        "K = torch.matmul(X, W_K)\n",
        "V = torch.matmul(X, W_V)\n",
        "\n",
        "# Compute attention scores\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "# Apply softmax to get attention weights\n",
        "attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "# Compute the attention output\n",
        "attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "print(\"Input Embeddings:\\n\", X)\n",
        "print(\"Queries (Q):\\n\", Q)\n",
        "print(\"Keys (K):\\n\", K)\n",
        "print(\"Values (V):\\n\", V)\n",
        "print(\"Attention Scores:\\n\", scores)\n",
        "print(\"Attention Weights:\\n\", attention_weights)\n",
        "print(\"Attention Output:\\n\", attention_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input matrices (batch size = 1, sequence length = 2, embedding size = 4)\n",
        "Q = torch.tensor([[1.0, 0.0, 1.0, 0.0],\n",
        "                  [0.0, 2.0, 0.0, 1.0]])\n",
        "\n",
        "K = torch.tensor([[1.0, 0.0, 1.0, 0.0],\n",
        "                  [0.0, 2.0, 0.0, 1.0]])\n",
        "\n",
        "V = torch.tensor([[0.5, 1.0, 0.5, 1.0],\n",
        "                  [1.0, 0.5, 1.0, 0.5]])\n",
        "\n",
        "# Dimensionality of the keys\n",
        "d_k = K.shape[-1]\n",
        "\n",
        "# Compute the dot products of Q and K transpose\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "\n",
        "# Scale the scores by the square root of d_k\n",
        "scaled_scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "# Apply the softmax function to get the attention weights\n",
        "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
        "\n",
        "# Compute the output by multiplying the attention weights with V\n",
        "output = torch.matmul(attention_weights, V)\n",
        "\n",
        "print(\"Queries (Q):\\n\", Q)\n",
        "print(\"Keys (K):\\n\", K)\n",
        "print(\"Values (V):\\n\", V)\n",
        "print(\"Dot-Product Scores:\\n\", scores)\n",
        "print(\"Scaled Scores:\\n\", scaled_scores)\n",
        "print(\"Attention Weights:\\n\", attention_weights)\n",
        "print(\"Output:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeNTT5TiqmeB",
        "outputId": "e3ded8db-e0c8-489b-f580-d260a1de062e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries (Q):\n",
            " tensor([[1., 0., 1., 0.],\n",
            "        [0., 2., 0., 1.]])\n",
            "Keys (K):\n",
            " tensor([[1., 0., 1., 0.],\n",
            "        [0., 2., 0., 1.]])\n",
            "Values (V):\n",
            " tensor([[0.5000, 1.0000, 0.5000, 1.0000],\n",
            "        [1.0000, 0.5000, 1.0000, 0.5000]])\n",
            "Dot-Product Scores:\n",
            " tensor([[2., 0.],\n",
            "        [0., 5.]])\n",
            "Scaled Scores:\n",
            " tensor([[1.0000, 0.0000],\n",
            "        [0.0000, 2.5000]])\n",
            "Attention Weights:\n",
            " tensor([[0.7311, 0.2689],\n",
            "        [0.0759, 0.9241]])\n",
            "Output:\n",
            " tensor([[0.6345, 0.8655, 0.6345, 0.8655],\n",
            "        [0.9621, 0.5379, 0.9621, 0.5379]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Define linear projections for Q, K, V for each head\n",
        "        self.W_Q = torch.nn.Linear(d_model, d_model)\n",
        "        self.W_K = torch.nn.Linear(d_model, d_model)\n",
        "        self.W_V = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Define linear projection for the output\n",
        "        self.W_O = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, X):\n",
        "        batch_size, seq_length, d_model = X.size()\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.W_Q(X)\n",
        "        K = self.W_K(X)\n",
        "        V = self.W_V(X)\n",
        "\n",
        "        # Split into multiple heads\n",
        "        Q = Q.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention for each head\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # Concatenate heads and apply final linear projection\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model)\n",
        "        output = self.W_O(attention_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Define the input embeddings (batch size = 1, sequence length = 2, embedding size = 4)\n",
        "X = torch.tensor([[[1.0, 0.0, 1.0, 0.0],\n",
        "                   [0.0, 2.0, 0.0, 1.0]]])\n",
        "\n",
        "# Define the multi-head attention module (embedding size = 4, number of heads = 2)\n",
        "mha = MultiHeadAttention(d_model=4, num_heads=2)\n",
        "\n",
        "# Forward pass through the multi-head attention\n",
        "output = mha(X)\n",
        "\n",
        "print(\"Input Embeddings:\\n\", X)\n",
        "print(\"Output of Multi-Head Attention:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRziYRD_rAyG",
        "outputId": "a442ffce-547b-493a-d353-791d642f21f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Embeddings:\n",
            " tensor([[[1., 0., 1., 0.],\n",
            "         [0., 2., 0., 1.]]])\n",
            "Output of Multi-Head Attention:\n",
            " tensor([[[-0.6001, -0.4562, -0.4179, -0.0301],\n",
            "         [-0.5855, -0.4454, -0.3797, -0.0267]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Create a matrix of [max_len, d_model] with positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define the input embeddings (batch size = 1, sequence length = 3, embedding size = 4)\n",
        "X = torch.tensor([[[1.0, 0.0, 1.0, 0.0],\n",
        "                   [0.0, 2.0, 0.0, 1.0],\n",
        "                   [1.0, 2.0, 1.0, 0.0]]])\n",
        "\n",
        "# Define the positional encoding module (embedding size = 4)\n",
        "pe = PositionalEncoding(d_model=4, max_len=10)\n",
        "\n",
        "# Forward pass through the positional encoding\n",
        "output = pe(X)\n",
        "\n",
        "print(\"Input Embeddings:\\n\", X)\n",
        "print(\"Positional Encodings:\\n\", pe.pe[0, :3, :])\n",
        "print(\"Output with Positional Encoding:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_qqjzXcrE-V",
        "outputId": "fa44df31-99aa-4a93-9704-c7a5ea9c61ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Embeddings:\n",
            " tensor([[[1., 0., 1., 0.],\n",
            "         [0., 2., 0., 1.],\n",
            "         [1., 2., 1., 0.]]])\n",
            "Positional Encodings:\n",
            " tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
            "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
            "        [ 0.9093, -0.4161,  0.0200,  0.9998]])\n",
            "Output with Positional Encoding:\n",
            " tensor([[[1.0000, 1.0000, 1.0000, 1.0000],\n",
            "         [0.8415, 2.5403, 0.0100, 1.9999],\n",
            "         [1.9093, 1.5839, 1.0200, 0.9998]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForwardNetwork(torch.nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = torch.nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# Define the input embeddings (batch size = 1, sequence length = 3, embedding size = 4)\n",
        "X = torch.tensor([[[1.0, 0.0, 1.0, 0.0],\n",
        "                   [0.0, 2.0, 0.0, 1.0],\n",
        "                   [1.0, 2.0, 1.0, 0.0]]])\n",
        "\n",
        "# Define the feed-forward network (embedding size = 4, feed-forward size = 8)\n",
        "ffn = FeedForwardNetwork(d_model=4, d_ff=8)\n",
        "\n",
        "# Forward pass through the feed-forward network\n",
        "output = ffn(X)\n",
        "\n",
        "print(\"Input Embeddings:\\n\", X)\n",
        "print(\"Output of Feed-Forward Network:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M5H-_Q9r-al",
        "outputId": "e7c82446-8f85-4233-fc1b-458763e9e404"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Embeddings:\n",
            " tensor([[[1., 0., 1., 0.],\n",
            "         [0., 2., 0., 1.],\n",
            "         [1., 2., 1., 0.]]])\n",
            "Output of Feed-Forward Network:\n",
            " tensor([[[-0.1774, -0.1202,  0.1628,  0.0524],\n",
            "         [-0.3373, -0.0402,  0.2143,  0.1117],\n",
            "         [-0.3009, -0.0594,  0.2037,  0.0570]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}