{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb+UCknyaNAPZNMdfpnjxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmepriyabrata/priyabrata_ai_python/blob/main/Optimization_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent"
      ],
      "metadata": {
        "id": "1qncDD_HoUTI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckZJcGlEoRL8",
        "outputId": "4f456acf-0359-464d-d0d2-82012aa3b9ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned weights: [-1.45239356e+119 -1.45239356e+119]\n",
            "Learned bias: -4.892786087589406e+118\n",
            "Predicted values: [-3.39406574e+119 -6.29885287e+119 -9.20363999e+119 -1.21084271e+120]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "  \"\"\"Calculates the mean squared error between true and predicted values.\"\"\"\n",
        "  return np.sum((y_true - y_pred) ** 2) / len(y_true)\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, num_iters):\n",
        "  \"\"\"\n",
        "  Implements gradient descent to find weights and bias for linear regression.\n",
        "\n",
        "  Args:\n",
        "      X: A numpy array of shape (num_samples, num_features) containing training data.\n",
        "      y: A numpy array of shape (num_samples,) containing target values.\n",
        "      learning_rate: The learning rate for gradient descent updates.\n",
        "      num_iters: The number of iterations to run gradient descent.\n",
        "\n",
        "  Returns:\n",
        "      A tuple containing the learned weights and bias.\n",
        "  \"\"\"\n",
        "\n",
        "  weights = np.random.rand(X.shape[1])\n",
        "  bias = 0\n",
        "\n",
        "  for _ in range(num_iters):\n",
        "    y_predicted = np.dot(X, weights) + bias\n",
        "\n",
        "    gradient_weights = -(2/X.shape[0]) * np.dot(X.T, (y_predicted - y))\n",
        "    gradient_bias = -(2/X.shape[0]) * np.sum(y_predicted - y)\n",
        "\n",
        "    weights -= learning_rate * gradient_weights\n",
        "    bias -= learning_rate * gradient_bias\n",
        "\n",
        "  return weights, bias\n",
        "\n",
        "X = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])\n",
        "y = np.array([2, 4, 5, 6])\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_iters = 1000\n",
        "\n",
        "weights, bias = gradient_descent(X, y, learning_rate, num_iters)\n",
        "\n",
        "print(\"Learned weights:\", weights)\n",
        "print(\"Learned bias:\", bias)\n",
        "\n",
        "y_predicted = np.dot(X, weights) + bias\n",
        "print(\"Predicted values:\", y_predicted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient"
      ],
      "metadata": {
        "id": "qCxthbbeogWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates the mean squared error between true and predicted values.\n",
        "\n",
        "  Args:\n",
        "    y_true: The ground truth labels (numpy array).\n",
        "    y_pred: The predicted labels (numpy array).\n",
        "\n",
        "  Returns:\n",
        "    The mean squared error (float).\n",
        "  \"\"\"\n",
        "  return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def gradient(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Calculates the gradient of the mean squared error for linear regression.\n",
        "\n",
        "  Args:\n",
        "    X: The training features (numpy array).\n",
        "    y: The training labels (numpy array).\n",
        "    w: The weights (numpy array).\n",
        "    b: The bias (float).\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the gradients of the weights and bias (numpy arrays).\n",
        "  \"\"\"\n",
        "  n = len(y)\n",
        "  predictions = X.dot(w) + b\n",
        "  errors = predictions - y\n",
        "  weight_gradient = (1/n) * X.T.dot(errors)\n",
        "  bias_gradient = (1/n) * np.sum(errors)\n",
        "  return weight_gradient, bias_gradient\n",
        "\n",
        "def stochastic_gradient_descent(X, y, learning_rate, n_epochs, batch_size=1):\n",
        "  \"\"\"\n",
        "  Performs stochastic gradient descent for linear regression.\n",
        "\n",
        "  Args:\n",
        "    X: The training features (numpy array).\n",
        "    y: The training labels (numpy array).\n",
        "    learning_rate: The learning rate (float).\n",
        "    n_epochs: The number of epochs (int).\n",
        "    batch_size: The size of mini-batches (int, default 1 for full batch SGD).\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the trained weights and bias (numpy arrays).\n",
        "  \"\"\"\n",
        "  n, d = X.shape\n",
        "  w = np.zeros(d)\n",
        "  b = 0\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    # Shuffle data for each epoch (optional for better convergence)\n",
        "    X_shuffled, y_shuffled = X.copy(), y.copy()\n",
        "    np.random.seed(epoch)\n",
        "    np.random.shuffle(X_shuffled)\n",
        "    np.random.shuffle(y_shuffled)\n",
        "\n",
        "    for i in range(0, n, batch_size):\n",
        "      X_batch = X_shuffled[i:i+batch_size]\n",
        "      y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "      weight_gradient, bias_gradient = gradient(X_batch, y_batch, w, b)\n",
        "      w -= learning_rate * weight_gradient\n",
        "      b -= learning_rate * bias_gradient\n",
        "\n",
        "  return w, b\n",
        "\n",
        "X = np.array([[1], [2], [3], [4]])\n",
        "y = np.array([2, 4, 5, 6])\n",
        "learning_rate = 0.01\n",
        "n_epochs = 1000\n",
        "\n",
        "w, b = stochastic_gradient_descent(X, y, learning_rate, n_epochs)\n",
        "\n",
        "y_pred = X.dot(w) + b\n",
        "print(f\"Predicted labels: {y_pred}\")\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "print(f\"Mean squared error: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAcmO0R2o2CQ",
        "outputId": "85e69ea1-93d8-4df2-a9a9-fa88ee8e175d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels: [4.14820039 4.28506891 4.42193744 4.55880596]\n",
            "Mean squared error: 1.7768064472147336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam"
      ],
      "metadata": {
        "id": "ze4fhuQYpW5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Adam:\n",
        "  \"\"\"\n",
        "  Stochastic Adam optimizer for machine learning.\n",
        "  \"\"\"\n",
        "  def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    Initializes the Adam optimizer with hyperparameters.\n",
        "\n",
        "    Args:\n",
        "      learning_rate (float): The base learning rate. Defaults to 0.001.\n",
        "      beta1 (float): The exponential decay rate for the first moment estimate. Defaults to 0.9.\n",
        "      beta2 (float): The exponential decay rate for the second moment estimate. Defaults to 0.999.\n",
        "      epsilon (float): A small constant for numerical stability. Defaults to 1e-8.\n",
        "    \"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.m = None\n",
        "    self.v = None\n",
        "    self.t = 0\n",
        "\n",
        "  def update(self, params, grads):\n",
        "    \"\"\"\n",
        "    Updates the parameters using the Adam optimization algorithm.\n",
        "\n",
        "    Args:\n",
        "      params (list): A list of NumPy arrays representing the model parameters.\n",
        "      grads (list): A list of NumPy arrays representing the gradients of the loss function with respect to the parameters.\n",
        "\n",
        "    Returns:\n",
        "      list: A list of updated NumPy arrays representing the model parameters.\n",
        "    \"\"\"\n",
        "    self.t += 1\n",
        "    if self.m is None:\n",
        "      self.m = [np.zeros_like(p) for p in params]\n",
        "      self.v = [np.zeros_like(p) for p in params]\n",
        "\n",
        "    for i, (param, grad) in enumerate(zip(params, grads)):\n",
        "      self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "      self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n",
        "      m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
        "      v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
        "      param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "    return params\n",
        "\n",
        "learning_rate = 0.01\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "\n",
        "params = [np.random.rand(10), np.random.rand(5)]\n",
        "grads = [np.random.rand(10), np.random.rand(5)]\n",
        "\n",
        "optimizer = Adam(learning_rate, beta1, beta2)\n",
        "updated_params = optimizer.update(params, grads)\n",
        "\n",
        "print(\"Updated parameters:\", updated_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTolCSgnq7_J",
        "outputId": "af2e4dcf-70bf-4ea2-cf82-733f7480848e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated parameters: [array([0.62968144, 0.08092526, 0.32222568, 0.41738095, 0.54438581,\n",
            "       0.61812652, 0.68739294, 0.77994969, 0.12189035, 0.33277045]), array([0.19155961, 0.69732423, 0.02339926, 0.89925004, 0.39516067])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSprop"
      ],
      "metadata": {
        "id": "fIM3PwoYrTor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(params, grads, learning_rate, rho, eps):\n",
        "  \"\"\"\n",
        "  Stochastic RMSprop optimization algorithm.\n",
        "\n",
        "  Args:\n",
        "      params: List of numpy arrays containing the model parameters.\n",
        "      grads: List of numpy arrays containing the gradients of the loss function.\n",
        "      learning_rate: Learning rate for the optimization.\n",
        "      rho: Decay rate for the moving average of squared gradients.\n",
        "      eps: Epsilon for numerical stability.\n",
        "\n",
        "  Returns:\n",
        "      Updated list of model parameters.\n",
        "  \"\"\"\n",
        "  updated_params = []\n",
        "  for param, grad in zip(params, grads):\n",
        "    if not hasattr(param, 's'):\n",
        "      param.s = np.zeros_like(param)\n",
        "    param.s = rho * param.s + (1 - rho) * np.square(grad)\n",
        "    updated_param = param - learning_rate * grad / (np.sqrt(param.s) + eps)\n",
        "    updated_params.append(updated_param)\n",
        "  return updated_params\n"
      ],
      "metadata": {
        "id": "crjH9PsrsPpS"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}