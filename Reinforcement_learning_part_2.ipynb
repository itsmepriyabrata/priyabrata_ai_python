{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+ePaoLpWca3CYtHvia3GH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmepriyabrata/priyabrata_ai_python/blob/main/Reinforcement_learning_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "policy gradients"
      ],
      "metadata": {
        "id": "d8w19kU1OzSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R9xbRmG8OHFt"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
        "        self.fc2 = nn.Linear(128, env.action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the REINFORCE algorithm\n",
        "class REINFORCE:\n",
        "    def __init__(self, policy_network):\n",
        "        self.policy_network = policy_network\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.001)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action_probs = self.policy_network(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action = torch.tensor(action, dtype=torch.long)\n",
        "        reward = torch.tensor(reward, dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "\n",
        "        action_probs = self.policy_network(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        loss = -log_prob * reward\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "class ActorCritic:\n",
        "    def __init__(self, policy_network, value_network):\n",
        "        self.policy_network = policy_network\n",
        "        self.value_network = value_network\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.001)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action_probs = self.policy_network(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action = torch.tensor(action, dtype=torch.long)\n",
        "        reward = torch.tensor(reward, dtype=torch.float)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "\n",
        "        action_probs = self.policy_network(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        loss = -log_prob * reward\n",
        "\n",
        "        value = self.value_network(state)\n",
        "        value_loss = (value - reward) ** 2\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "reinforce = REINFORCE(PolicyNetwork())\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    while not done:\n",
        "        action = reinforce.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        reinforce.update(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "    print(f'Episode {episode+1}, Reward: {rewards}')\n",
        "\n",
        "actor_critic = ActorCritic(PolicyNetwork(), PolicyNetwork())\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    while not done:\n",
        "        action = actor_critic.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        actor_critic.update(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "    print(f'Episode {episode+1}, Reward: {rewards}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor-Critic methods**"
      ],
      "metadata": {
        "id": "jZOwG5lNPUJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
        "        self.fc_actor = nn.Linear(128, env.action_space.n)\n",
        "        self.fc_critic = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        action_probs = torch.softmax(self.fc_actor(x), dim=-1)\n",
        "        value = self.fc_critic(x)\n",
        "        return action_probs, value\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
        "        self.gamma = 0.99\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action_probs, _ = self.network(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def update(self, states, actions, rewards, next_states):\n",
        "        states = torch.tensor(states, dtype=torch.float)\n",
        "        actions = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float)\n",
        "\n",
        "        action_probs, values = self.network(states)\n",
        "        dist = Categorical(action_probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "\n",
        "        next_action_probs, next_values = self.network(next_states)\n",
        "        next_values = next_values.detach()\n",
        "\n",
        "        td_targets = rewards + self.gamma * next_values\n",
        "        td_errors = td_targets - values\n",
        "\n",
        "        actor_loss = -log_probs * td_errors.detach()\n",
        "        critic_loss = td_errors ** 2\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.2\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action_probs, _ = self.network(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def update(self, states, actions, rewards, next_states, old_log_probs):\n",
        "        states = torch.tensor(states, dtype=torch.float)\n",
        "        actions = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float)\n",
        "        old_log_probs = torch.tensor(old_log_probs, dtype=torch.float)\n",
        "\n",
        "        action_probs, values = self.network(states)\n",
        "        dist = Categorical(action_probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "\n",
        "        next_action_probs, next_values = self.network(next_states)\n",
        "        next_values = next_values.detach()\n",
        "\n",
        "        td_targets = rewards + self.gamma * next_values\n",
        "        td_errors = td_targets - values\n",
        "\n",
        "        ratios = torch.exp(log_probs - old_log_probs)\n",
        "        surr1 = ratios * td_errors\n",
        "        surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * td_errors\n",
        "\n",
        "        actor_loss = -torch.min(surr1, surr2)\n",
        "        critic_loss = td_errors ** 2\n",
        "\n",
        "        loss = actor_loss + critic_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "a2c = A2C(ActorCriticNetwork())\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    while not done:\n",
        "        action = a2c.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        a2c.update([state], [action], [reward], [next_state])\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "    print(f'Episode {episode+1}, Reward: {rewards}')\n",
        "\n",
        "ppo = PPO(ActorCriticNetwork())\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    states, actions, rewards, next_states, old_log_probs = [], [], [], [], []\n",
        "    while not done:\n",
        "        action = ppo.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        next_states.append(next_state)\n",
        "        action_probs, _ = ppo.network(torch.tensor(state, dtype=torch.float))\n",
        "        dist = Categorical(action_probs)\n",
        "        old_log_probs.append(dist.log_prob(torch.tensor(action, dtype=torch.long)))\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "    ppo.update(states, actions, rewards, next_states, old_log_probs)\n",
        "    print(f'Episode {episode+1}, Reward: {rewards}')\n"
      ],
      "metadata": {
        "id": "PA_ixcvvPQj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monte Carlo Tree Search"
      ],
      "metadata": {
        "id": "kPX3yedSQKl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent=None):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0\n",
        "\n",
        "    def add_child(self, child):\n",
        "        self.children.append(child)\n",
        "\n",
        "    def update(self, value):\n",
        "        self.visits += 1\n",
        "        self.value += value\n",
        "\n",
        "    def get_value(self):\n",
        "        return self.value / self.visits\n",
        "\n",
        "    def get_ucb(self, parent):\n",
        "        return self.get_value() + math.sqrt(math.log(parent.visits) / self.visits)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, root_state):\n",
        "        self.root = Node(root_state)\n",
        "\n",
        "    def select(self, node):\n",
        "        while node.children:\n",
        "            node = max(node.children, key=lambda child: child.get_ucb(node))\n",
        "        return node\n",
        "\n",
        "    def expand(self, node):\n",
        "        if node.state.is_terminal():\n",
        "            return\n",
        "        for action in node.state.get_actions():\n",
        "            child_state = node.state.apply_action(action)\n",
        "            child = Node(child_state, node)\n",
        "            node.add_child(child)\n",
        "\n",
        "    def simulate(self, node):\n",
        "        state = node.state\n",
        "        while not state.is_terminal():\n",
        "            action = random.choice(state.get_actions())\n",
        "            state = state.apply_action(action)\n",
        "        return state.get_value()\n",
        "\n",
        "    def backpropagate(self, node, value):\n",
        "        while node:\n",
        "            node.update(value)\n",
        "            node = node.parent\n",
        "\n",
        "    def run(self, iterations):\n",
        "        for _ in range(iterations):\n",
        "            node = self.root\n",
        "            while True:\n",
        "                node = self.select(node)\n",
        "                if not node.children:\n",
        "                    break\n",
        "            self.expand(node)\n",
        "            value = self.simulate(node)\n",
        "            self.backpropagate(node, value)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        node = self.root\n",
        "        while True:\n",
        "            node = self.select(node)\n",
        "            if not node.children:\n",
        "                break\n",
        "        return node.children[0].state.get_action()\n",
        "\n",
        "# Example usage:\n",
        "class Game:\n",
        "    def __init__(self):\n",
        "        self.state = 'start'\n",
        "\n",
        "    def get_actions(self):\n",
        "        if self.state == 'start':\n",
        "            return ['A', 'B']\n",
        "        elif self.state == 'A':\n",
        "            return ['C', 'D']\n",
        "        elif self.state == 'B':\n",
        "            return ['E', 'F']\n",
        "        elif self.state == 'C':\n",
        "            return ['G', 'H']\n",
        "        elif self.state == 'D':\n",
        "            return ['I', 'J']\n",
        "        elif self.state == 'E':\n",
        "            return ['K', 'L']\n",
        "        elif self.state == 'F':\n",
        "            return ['M', 'N']\n",
        "        elif self.state == 'G':\n",
        "            return ['O', 'P']\n",
        "        elif self.state == 'H':\n",
        "            return ['Q', 'R']\n",
        "        elif self.state == 'I':\n",
        "            return ['S', 'T']\n",
        "        elif self.state == 'J':\n",
        "            return ['U', 'V']\n",
        "        elif self.state == 'K':\n",
        "            return ['W', 'X']\n",
        "        elif self.state == 'L':\n",
        "            return ['Y', 'Z']\n",
        "        elif self.state == 'M':\n",
        "            return ['AA', 'AB']\n",
        "        elif self.state == 'N':\n",
        "            return ['AC', 'AD']\n",
        "        elif self.state == 'O':\n",
        "            return ['AE', 'AF']\n",
        "        elif self.state == 'P':\n",
        "            return ['AG', 'AH']\n",
        "        elif self.state == 'Q':\n",
        "            return ['AI', 'AJ']\n",
        "        elif self.state == 'R':\n",
        "            return ['AK', 'AL']\n",
        "        elif self.state == 'S':\n",
        "            return ['AM', 'AN']\n",
        "        elif self.state == 'T':\n",
        "            return ['AO', 'AP']\n",
        "        elif self.state == 'U':\n",
        "            return ['AQ', 'AR']\n",
        "        elif self.state == 'V':\n",
        "            return ['AS', 'AT']\n",
        "        elif self.state == 'W':\n",
        "            return ['AU', 'AV']\n",
        "        elif self.state == 'X':\n",
        "            return ['AW', 'AX']\n",
        "        elif self.state == 'Y':\n",
        "            return ['AY', 'AZ']\n",
        "        elif self.state == 'Z':\n",
        "            return ['BA', 'BB']\n",
        "        elif self.state == 'AA':\n",
        "            return ['BC', 'BD']\n",
        "        elif self.state == 'AB':\n",
        "            return ['BE', 'BF']\n",
        "        elif self.state == 'AC':\n",
        "            return ['BG', 'BH']\n",
        "        elif self.state == 'AD':\n",
        "            return ['BI', 'BJ']\n",
        "        elif self.state == 'AE':\n",
        "            return ['BK', 'BL']\n",
        "        elif self.state == 'AF':\n",
        "            return ['BM', 'BN']\n",
        "        elif self.state == 'AG':\n",
        "            return ['BO', 'BP']\n",
        "        elif self.state == 'AH':\n",
        "            return ['BQ', 'BR']\n",
        "        elif self.state == 'AI':\n",
        "            return ['BS', 'BT']\n",
        "        elif self.state == 'AJ':\n",
        "            return ['BU', 'BV']\n",
        "        elif self.state == 'AK':\n",
        "            return ['BW', 'BX']\n",
        "        elif self.state == 'AL':\n",
        "            return ['BY', 'BZ']\n",
        "        elif self.state == 'AM':\n",
        "            return ['CA', 'CB']\n",
        "        elif self.state == 'AN':\n",
        "            return ['CC', 'CD']\n",
        "        elif self.state == 'AO':\n",
        "            return ['CE', 'CF']\n",
        "        elif self.state == 'AP':\n",
        "            return ['CG', 'CH']\n",
        "        elif self.state == 'AQ':\n",
        "            return ['CI', 'CJ']\n",
        "        elif self.state == 'AR':\n",
        "            return ['CK', 'CL']\n",
        "        elif self.state == 'AS':\n",
        "            return ['CM', 'CN']\n",
        "        elif self.state == 'AT':\n",
        "            return ['CO', 'CP']\n",
        "        elif self.state == 'AU':\n",
        "            return ['CQ', 'CR']\n",
        "        elif self.state == 'AV':\n",
        "            return ['CS', 'CT']\n",
        "        elif self.state == 'AW':\n",
        "            return ['CU', 'CV']\n",
        "        elif self.state == 'AX':\n",
        "            return ['CW', 'CX']\n",
        "        elif self.state == 'AY':\n",
        "            return ['CY', 'CZ']\n",
        "        elif self.state == 'AZ':\n",
        "            return ['DA', 'DB']\n",
        "        elif self.state == 'BA':\n",
        "            return ['DC', 'DD']\n",
        "        elif self.state == 'BB':\n",
        "            return ['DE', 'DF']\n",
        "        elif self.state == 'BC':\n",
        "            return ['DG', 'DH']\n",
        "        elif self.state == 'BD':\n",
        "            return ['DI', 'DJ']\n",
        "        elif self.state == 'BE':\n",
        "            return ['DK', 'DL']\n",
        "        elif self.state == 'BF':\n",
        "            return ['DM', 'DN']\n",
        "        elif self.state == 'BG':\n",
        "            return ['DO', 'DP']\n",
        "        elif self.state == 'BH':\n",
        "            return ['DQ', 'DR']\n",
        "        elif self.state == 'BI':\n",
        "            return ['DS', 'DT']\n",
        "        elif self.state == 'BJ':\n",
        "            return ['DU', 'DV']\n",
        "        elif self.state == 'BK':\n",
        "            return ['DW', 'DX']\n",
        "        elif self.state == 'BL':\n",
        "            return ['DY', 'DZ']\n",
        "        elif self.state == 'BM':\n",
        "            return ['EA', 'EB']\n",
        "        elif self.state == 'BN':\n",
        "            return ['EC', 'ED']\n",
        "        elif self.state == 'BO':\n",
        "            return ['EE', 'EF']\n",
        "        elif self.state == 'BP':\n",
        "            return ['EG', 'EH']\n",
        "        elif self.state == 'BQ':\n",
        "            return ['EI', 'EJ']\n",
        "        elif self.state == 'BR':\n",
        "            return ['EK', 'EL']\n",
        "        elif self.state == 'BS':\n",
        "            return ['EM', 'EN']\n",
        "        elif self.state == 'BT':\n",
        "            return ['EO', 'EP']\n",
        "        elif self.state == 'BU':\n",
        "            return ['EQ', 'ER']\n",
        "        elif self.state == 'BV':\n",
        "            return ['ES', 'ET']\n",
        "        elif self.state == 'BW':\n",
        "            return ['EU', 'EV']\n",
        "        elif self.state == 'BX':\n",
        "            return ['EW', 'EX']\n",
        "        elif self.state == 'BY':\n",
        "            return ['EY', 'EZ']\n",
        "        elif self.state == 'BZ':\n",
        "            return ['FA', 'FB']\n",
        "        elif self.state == 'BA':\n",
        "            return ['FC', 'FD']\n",
        "        elif self.state == 'BB':\n",
        "            return ['FE', 'FF']\n",
        "        elif self.state == 'BC':\n",
        "            return ['FG', 'FH']\n",
        "        elif self.state == 'BD':\n",
        "            return ['FI', 'FJ']\n",
        "        elif self.state == 'BE':\n",
        "            return ['FK', 'FL']\n",
        "        elif self.state == 'BF':\n",
        "            return ['FM', 'FN']\n",
        "        elif self.state == 'BG':\n",
        "            return ['FO', 'FP']\n",
        "        elif self.state == 'BH':\n",
        "            return ['FQ', 'FR']\n",
        "        elif self.state == 'BI':\n",
        "            return ['FS', 'FT']\n",
        "        elif self.state == 'BJ':\n",
        "            return ['FU', 'FV']\n",
        "        elif self.state == 'BK':\n",
        "            return ['FW', 'FX']\n",
        "        elif self.state == 'BL':\n",
        "            return ['FY', 'FZ']\n",
        "        elif self.state == 'BM':\n",
        "            return ['GA', 'GB']\n",
        "        elif self.state == 'BN':\n",
        "            return ['GC', 'GD']\n",
        "        elif self.state == 'BO':\n",
        "            return ['GE', 'GF']\n",
        "        elif self.state == 'BP':\n",
        "            return ['GG', 'GH']\n",
        "        elif self.state == 'BQ':\n",
        "            return ['GI', 'GJ']\n",
        "        elif self.state == 'BR':\n",
        "            return ['GK', 'GL']\n",
        "        elif self.state == 'BS':\n",
        "            return ['GM', 'GN']\n",
        "        elif self.state == 'BT':\n",
        "            return ['GO', 'GP']\n",
        "        elif self.state == 'BU':\n",
        "            return ['GQ', 'GR']\n",
        "        elif self.state == 'BV':\n",
        "            return ['GS', 'GT']\n",
        "        elif self.state == 'BW':\n",
        "            return ['GU', 'GV']\n",
        "        elif self.state == 'BX':\n",
        "            return ['GW', 'GX']\n",
        "        elif self.state == 'BY':\n",
        "            return ['GY', 'GZ']\n",
        "        elif self.state == 'BZ':\n",
        "            return ['HA', 'HB']\n",
        "        elif self.state == 'BA':\n",
        "            return ['HC', 'HD']\n",
        "        elif self.state == 'BB':\n",
        "            return ['HE', 'HF']\n",
        "        elif self.state == 'BC':\n",
        "            return ['HG', 'HH']\n",
        "        elif self.state == 'BD':\n",
        "            return ['HI', 'HJ']\n",
        "        elif self.state == 'BE':\n",
        "            return ['HK', 'HL']\n",
        "        elif self.state == 'BF':\n",
        "            return ['HM', 'HN']\n",
        "        elif self.state == 'BG':\n",
        "            return ['HO', 'HP']\n",
        "        elif self.state == 'BH':\n",
        "            return ['HQ', 'HR']\n",
        "        elif self.state == 'BI':\n",
        "            return ['HS', 'HT']\n",
        "        elif self.state == 'BJ':\n",
        "            return ['HU', 'HV']\n",
        "        elif self.state == 'BK':\n",
        "            return ['HW', 'HX']\n",
        "        elif self.state == 'BL':\n",
        "            return ['HY', 'HZ']\n",
        "        elif self.state == 'BM':\n",
        "            return ['IA', 'IB']\n",
        "        elif self.state == 'BN':\n",
        "            return ['IC', 'ID']\n",
        "        elif self.state == 'BO':\n",
        "            return"
      ],
      "metadata": {
        "id": "NL6v2KTEEeA9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}