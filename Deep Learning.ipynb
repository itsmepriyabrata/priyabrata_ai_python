{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuGEBSToYYATBGcQuDNctn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmepriyabrata/priyabrata_ai_python/blob/main/Deep%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convolutional neural networks"
      ],
      "metadata": {
        "id": "OocB9UOcXfKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6XTC3bMUCLF",
        "outputId": "ee46c800-3e5b-4311-c053-7d6faf079873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "938/938 [==============================] - 58s 60ms/step - loss: 0.1808 - accuracy: 0.9457 - val_loss: 0.0429 - val_accuracy: 0.9860\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 52s 55ms/step - loss: 0.0493 - accuracy: 0.9853 - val_loss: 0.0371 - val_accuracy: 0.9881\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 48s 52ms/step - loss: 0.0359 - accuracy: 0.9887 - val_loss: 0.0339 - val_accuracy: 0.9878\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 50s 53ms/step - loss: 0.0277 - accuracy: 0.9912 - val_loss: 0.0362 - val_accuracy: 0.9888\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 51s 55ms/step - loss: 0.0227 - accuracy: 0.9930 - val_loss: 0.0400 - val_accuracy: 0.9869\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f8dcc983580>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent neural networks"
      ],
      "metadata": {
        "id": "t32x68iAXkth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Generate some sample data\n",
        "X_train = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
        "y_train = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(units=4, input_shape=(2, 3), return_sequences=True))\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
        "\n",
        "# Make a prediction\n",
        "X_test = np.array([[[13, 14, 15], [16, 17, 18]]])\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Predicted output:\", y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poIIarfDXxm5",
        "outputId": "f512872b-9e88-456a-ccda-472c8e20ef70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 224ms/step\n",
            "Predicted output: [[[0.91500306 1.3744221 ]\n",
            "  [0.9932654  1.3598087 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Long short term memory network"
      ],
      "metadata": {
        "id": "0eWFrBwiX6_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Generate some sample data\n",
        "X_train = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
        "y_train = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=4, input_shape=(2, 3), return_sequences=True))\n",
        "model.add(LSTM(units=2))\n",
        "model.add(Dense(2, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=0)\n",
        "\n",
        "# Make a prediction\n",
        "X_test = np.array([[[13, 14, 15], [16, 17, 18]]])\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Predicted output:\", y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bloJUWBYLL5",
        "outputId": "0e78e19f-9ab8-479c-ff02-d245f1a8d956"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 877ms/step\n",
            "Predicted output: [[0.7996338 1.324119 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generative Adversarial Networks"
      ],
      "metadata": {
        "id": "zmUYjRHZYT0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "# Define the generator model\n",
        "def define_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, use_bias=False, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(128 * 7 * 7, use_bias=False))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    return model\n",
        "\n",
        "# Define the discriminator model\n",
        "def define_discriminator():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 3]))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the combined GAN model\n",
        "def define_gan(g_model, d_model):\n",
        "    model = Sequential()\n",
        "    model.add(g_model)\n",
        "    model.add(d_model)\n",
        "    return model\n",
        "\n",
        "# Define the loss functions\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = (real_loss + fake_loss) / 2\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def train(g_model, d_model, gan_model, latent_dim, dataset, epochs, batch_size):\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(dataset) // batch_size):\n",
        "            real_images = dataset[i * batch_size:(i + 1) * batch_size]\n",
        "            real_images = np.array(real_images) / 127.5 - 1.0\n",
        "\n",
        "            latent_vectors = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "            fake_images = g_model.predict(latent_vectors)\n",
        "\n",
        "            d_loss_real = discriminator_loss(d_model.predict(real_images), d_model.predict(real_images))\n",
        "            d_loss_fake = discriminator_loss(d_model.predict(fake_images), d_model.predict(fake_images))\n",
        "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "\n",
        "            g_loss = generator_loss(d_model.predict(g_model.predict(latent_vectors)))\n",
        "\n",
        "            print(f'Epoch {epoch + 1}, Batch {i + 1}, D Loss: {d_loss}, G Loss: {g_loss}')\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, _), (_, _) = mnist.load_data()\n",
        "dataset = x_train\n",
        "\n",
        "latent_dim = 100\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "generator = define_generator(latent_dim)\n",
        "discriminator = define_discriminator()\n",
        "gan = define_gan(generator, discriminator)\n",
        "\n",
        "generator.compile(optimizer=Adam(0.0002, 0.5), loss=generator_loss)\n",
        "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss=discriminator_loss)\n",
        "gan.compile(optimizer=Adam(0.0002, 0.5), loss=generator_loss)\n",
        "\n",
        "train(generator, discriminator, gan, latent_dim, dataset, epochs, batch_size)"
      ],
      "metadata": {
        "id": "3Rgo4COsZ4oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoders"
      ],
      "metadata": {
        "id": "Y9xdZhlIaSWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "\n",
        "# Define the Autoencoder model\n",
        "def define_autoencoder(input_dim, encoding_dim):\n",
        "    # Encoder\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "\n",
        "    # Decoder\n",
        "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "    # Autoencoder\n",
        "    autoencoder = Model(input_layer, decoded)\n",
        "\n",
        "    # Encoder model\n",
        "    encoder = Model(input_layer, encoded)\n",
        "\n",
        "    # Decoder model\n",
        "    encoded_input = Input(shape=(encoding_dim,))\n",
        "    decoder_layer = autoencoder.layers[-1]\n",
        "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "    return autoencoder, encoder, decoder\n",
        "\n",
        "# Generate some sample data\n",
        "input_dim = 784  # MNIST image size\n",
        "encoding_dim = 32  # Size of the encoded representation\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.reshape(x_train.shape[0], input_dim) / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], input_dim) / 255\n",
        "\n",
        "# Define and compile the Autoencoder model\n",
        "autoencoder, encoder, decoder = define_autoencoder(input_dim, encoding_dim)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the Autoencoder\n",
        "autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjOVBLjbaVcu",
        "outputId": "d6373560-6b65-46cf-e06f-6090de523c88"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.2776 - val_loss: 0.1914\n",
            "Epoch 2/10\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1715 - val_loss: 0.1529\n",
            "Epoch 3/10\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.1437 - val_loss: 0.1334\n",
            "Epoch 4/10\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.1284 - val_loss: 0.1209\n",
            "Epoch 5/10\n",
            "235/235 [==============================] - 4s 16ms/step - loss: 0.1178 - val_loss: 0.1121\n",
            "Epoch 6/10\n",
            "235/235 [==============================] - 3s 13ms/step - loss: 0.1103 - val_loss: 0.1060\n",
            "Epoch 7/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1053 - val_loss: 0.1020\n",
            "Epoch 8/10\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1018 - val_loss: 0.0990\n",
            "Epoch 9/10\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.0992 - val_loss: 0.0971\n",
            "Epoch 10/10\n",
            "235/235 [==============================] - 3s 12ms/step - loss: 0.0975 - val_loss: 0.0954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8dc7de0c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n",
            "313/313 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    }
  ]
}