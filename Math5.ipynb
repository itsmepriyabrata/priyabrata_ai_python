{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcDWEfvgPh9P3zLRFt+dQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsmepriyabrata/priyabrata_ai_python/blob/main/Math5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provided code calculates various entropy and cross-entropy values using both NumPy and PyTorch.The code doesn't create any new values; instead, it computes and prints various entropy and cross-entropy values using both NumPy and PyTorch.\n",
        "Note:Entropy, in the context of information theory and probability theory, is a measure of uncertainty or disorder associated with a random variable. It quantifies the average amount of information produced by a random variable or the average uncertainty inherent in a random process.\n",
        "\n"
      ],
      "metadata": {
        "id": "zbPnS9ftI1Qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hswvdizVGwNy",
        "outputId": "6e3bee3a-b5f7-4633-91f1-189181cf012f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong entropy: 0.34657359027997264\n",
            "Correct entropy: 0.5623351446188083\n",
            "Correct entropy: 0.5623351446188083\n",
            "Cross entropy: 1.3862943611198906\n",
            "Correct entropy: 1.3862943611198906\n",
            "Manually simplified: 1.3862943611198906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(75.)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "     # probability of an event happening\n",
        "p = .25\n",
        "\n",
        "# NOT the correct formula!\n",
        "H = -( p*np.log(p) )\n",
        "print('Wrong entropy: ' + str(H))\n",
        "# the correct way to compute entropy\n",
        "x = [.25,.75]\n",
        "\n",
        "H = 0\n",
        "for p in x:\n",
        "  H -= p*np.log(p)\n",
        "\n",
        "print('Correct entropy: ' + str(H))\n",
        "# also correct, written out for N=2 events\n",
        "H = -( p*np.log(p) + (1-p)*np.log(1-p) )\n",
        "print('Correct entropy: ' + str(H))\n",
        "# note: all probs must sum to 1!\n",
        "p = [   1,0   ] # sum=1\n",
        "q = [ .25,.75 ] # sum=1\n",
        "\n",
        "H = 0\n",
        "for i in range(len(p)):\n",
        "  H -= p[i]*np.log(q[i])\n",
        "\n",
        "print('Cross entropy: ' + str(H))\n",
        "# also correct, written out for N=2 events\n",
        "H = -( p[0]*np.log(q[0]) + p[1]*np.log(q[1]) )\n",
        "print('Correct entropy: ' + str(H))\n",
        "\n",
        "# simplification\n",
        "H = -np.log(q[0])\n",
        "print('Manually simplified: ' + str(H))\n",
        "# now using pytorch\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# note: inputs must be Tensors\n",
        "q_tensor = torch.Tensor(q)\n",
        "p_tensor = torch.Tensor(p)\n",
        "\n",
        "F.binary_cross_entropy(p_tensor, q_tensor)\n",
        ""
      ]
    }
  ]
}